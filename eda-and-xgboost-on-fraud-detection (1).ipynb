{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.\tObjective and Background\n\nIn this kernel, I work with IEEE Fraud Detection competition.\n\nEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry.\n\nSpecifically, **the objective of this kernel is to predict the probability that an online transaction is fraudulent**.\n\n\n*Acknowledgements to inspiring codebooks:*\n1. https://www.kaggle.com/artgor/eda-and-models/notebook\n2. https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt/notebook\n3. https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600","metadata":{}},{"cell_type":"markdown","source":"# 2.   Roadmap\n\na).\tPreparation: Import Libraries\n\nb).\tPreparation: Define Functions used in this kernel\n\nc).\tData Loading and Overview\n\nd).\tExploratory Data Analysis\n\ne).\tFeaturing Engineering\n\nf).\tFeature Selection\n\ng).\tXBGoost Modelling\n\nh).\tMake and Submit the Predication","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import roc_auc_score\npd.options.display.precision = 15\n\n\nimport xgboost as xgb\nimport time\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-13T11:38:27.43708Z","iopub.execute_input":"2021-11-13T11:38:27.437395Z","iopub.status.idle":"2021-11-13T11:38:32.800587Z","shell.execute_reply.started":"2021-11-13T11:38:27.437339Z","shell.execute_reply":"2021-11-13T11:38:32.799835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions used in this kernel","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-13T11:38:32.804071Z","iopub.execute_input":"2021-11-13T11:38:32.804314Z","iopub.status.idle":"2021-11-13T11:38:33.559541Z","shell.execute_reply.started":"2021-11-13T11:38:32.804266Z","shell.execute_reply":"2021-11-13T11:38:33.55875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data loading and overview\n\nData is separated into two datasets: information about the **customer identity** and **transaction**, joined by TransactionID. Not all transactions have corresponding identity information.\n\n- **Numerical Features - Transaction**\n    - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n    - TransactionAMT: transaction payment amount in USD\n    - dist: distance\n    - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n    - D1-D15: timedelta, such as days between previous transaction, etc.\n    - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n\n- **Categorical Features - Transaction**\n    - ProductCD: product code, the product for each transaction\n    - card1 - card6: : payment card information, such as card type, card category, issue bank, country, etc.\n    - addr1, addr2: address\n    - P_emaildomain: purchaser email domain\n    - R_emaildomain: recipient email domain\n    - M1 - M9: match, such as names on card and address, etc.\n\n\n\n- **Explanation on Identity Data**\n    - Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n    - They're collected by Vesta’s fraud protection system and digital security partners.(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n\n- **Categorical Features - Identity**\n    - DeviceType\n    - DeviceInfo\n    - id_12 - id_38\n\n*More details about the data: \nhttps://www.kaggle.com/c/ieee-fraud-detection/discussion/101203*","metadata":{}},{"cell_type":"markdown","source":"We will load all the data except 219 V columns that were determined redundant by correlation analysis https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id.","metadata":{}},{"cell_type":"code","source":"%%time\nfolder_path = '../input/'\n\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\n\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n\nfix = {o:n for o, n in zip(test_identity.columns, train_identity.columns)}#\"id\" columns in test dataset are different from the train dataset\ntest_identity.rename(columns=fix, inplace=True)\n\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\ndel train_identity, train_transaction, test_identity, test_transaction, fix\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:38:33.560986Z","iopub.execute_input":"2021-11-13T11:38:33.561257Z","iopub.status.idle":"2021-11-13T11:45:09.537107Z","shell.execute_reply.started":"2021-11-13T11:38:33.561213Z","shell.execute_reply":"2021-11-13T11:45:09.536303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:09.538409Z","iopub.execute_input":"2021-11-13T11:45:09.538718Z","iopub.status.idle":"2021-11-13T11:45:09.543514Z","shell.execute_reply.started":"2021-11-13T11:45:09.538644Z","shell.execute_reply":"2021-11-13T11:45:09.542689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\nuse_Vcols = ['V'+str(x) for x in v]\nVcols = ['V'+str(x) for x in range(1,340)]\ndrop_Vcols = list(set(Vcols) - set(use_Vcols))\n\ntrain.drop(drop_Vcols, axis=1, inplace=True)\ntest.drop(drop_Vcols, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:09.54716Z","iopub.execute_input":"2021-11-13T11:45:09.547655Z","iopub.status.idle":"2021-11-13T11:45:11.720341Z","shell.execute_reply.started":"2021-11-13T11:45:09.547601Z","shell.execute_reply":"2021-11-13T11:45:11.719622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:11.723178Z","iopub.execute_input":"2021-11-13T11:45:11.723453Z","iopub.status.idle":"2021-11-13T11:45:11.731049Z","shell.execute_reply.started":"2021-11-13T11:45:11.723408Z","shell.execute_reply":"2021-11-13T11:45:11.730277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have two medium-sized datasets with a lot of columns. Train and test data have similar number of rows","metadata":{}},{"cell_type":"code","source":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:11.73253Z","iopub.execute_input":"2021-11-13T11:45:11.733052Z","iopub.status.idle":"2021-11-13T11:45:13.386481Z","shell.execute_reply.started":"2021-11-13T11:45:11.733002Z","shell.execute_reply":"2021-11-13T11:45:13.385335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:13.388124Z","iopub.execute_input":"2021-11-13T11:45:13.388438Z","iopub.status.idle":"2021-11-13T11:45:19.452445Z","shell.execute_reply.started":"2021-11-13T11:45:13.388387Z","shell.execute_reply":"2021-11-13T11:45:19.451701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:19.453797Z","iopub.execute_input":"2021-11-13T11:45:19.454234Z","iopub.status.idle":"2021-11-13T11:45:19.461032Z","shell.execute_reply.started":"2021-11-13T11:45:19.45405Z","shell.execute_reply":"2021-11-13T11:45:19.460287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[one_value_cols_test].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:19.46252Z","iopub.execute_input":"2021-11-13T11:45:19.463218Z","iopub.status.idle":"2021-11-13T11:45:19.565312Z","shell.execute_reply.started":"2021-11-13T11:45:19.463164Z","shell.execute_reply":"2021-11-13T11:45:19.564578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of columns have missing data, which is normal in real world. Also there is one column in the test dataset with one unique value. \n\nThere are a lot of continuous variables and some categorical. Let's have a closer look at them.","metadata":{}},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis\n\nI will start EDA on identity data and transaction respectively. The aim is to answer some questions like:\n\n- What type of data we have on our data?\n- How many cols, rows, missing values we have?\n- What's the target distribution?\n- What's the Transactions values distribution of fraud and no fraud transactions?\n- Do we have predominant fraudulent products?\n- What features or target shows some interesting patterns?\n\nAnd a lot of more questions that will raise trought the exploration.","metadata":{}},{"cell_type":"markdown","source":"## 4.1 EDA on Identity Data\n\nLet's start with identity information.\nid_01 - id_11 are continuous variables, id_12 - id_38 are categorical and the last two columns are obviously also categorical.\n\n*Previously*:\n- **Explanation on Identity Data**\n    - Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n    - They're collected by Vesta’s fraud protection system and digital security partners.(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n\n- **Categorical Features - Identity**\n    - DeviceType\n    - DeviceInfo\n    - id_12 - id_38","metadata":{}},{"cell_type":"markdown","source":"### 4.1.1 EDA on Numerical Features (id_01 - id_11) - Identity","metadata":{"execution":{"iopub.status.busy":"2021-11-09T07:25:48.917213Z","iopub.execute_input":"2021-11-09T07:25:48.917936Z","iopub.status.idle":"2021-11-09T07:25:48.921727Z","shell.execute_reply.started":"2021-11-09T07:25:48.917881Z","shell.execute_reply":"2021-11-09T07:25:48.920921Z"}}},{"cell_type":"code","source":"train[['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11']].describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:19.566653Z","iopub.execute_input":"2021-11-13T11:45:19.566932Z","iopub.status.idle":"2021-11-13T11:45:20.567999Z","shell.execute_reply.started":"2021-11-13T11:45:19.566877Z","shell.execute_reply":"2021-11-13T11:45:20.567328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train['id_01'], bins=77);\nplt.title('Distribution of id_01 variable');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:20.569577Z","iopub.execute_input":"2021-11-13T11:45:20.569991Z","iopub.status.idle":"2021-11-13T11:45:21.059199Z","shell.execute_reply.started":"2021-11-13T11:45:20.569822Z","shell.execute_reply":"2021-11-13T11:45:21.058431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`id_01` has an interesting distribution: it has 77 unique non-positive values with skewness to 0.","metadata":{}},{"cell_type":"code","source":"s1 = train['id_03'].value_counts(dropna=False, normalize=True).head()\ns2 = train['id_04'].value_counts(dropna=False, normalize=True).head()\ns3 = train['id_05'].value_counts(dropna=False, normalize=True).head()\ns4 = train['id_06'].value_counts(dropna=False, normalize=True).head()\ns5 = train['id_09'].value_counts(dropna=False, normalize=True).head()\ns6 = train['id_10'].value_counts(dropna=False, normalize=True).head()\nprint(pd.concat([s1, s2, s3, s4, s5, s6], axis = 1))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:21.060498Z","iopub.execute_input":"2021-11-13T11:45:21.060959Z","iopub.status.idle":"2021-11-13T11:45:21.18536Z","shell.execute_reply.started":"2021-11-13T11:45:21.060906Z","shell.execute_reply":"2021-11-13T11:45:21.184576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`id_03`, `id_04`, `id_05`, `id_06`, `id_09` and `id_10` have over 76% of missing values and over 90% of values are either missing or equal to 0.\nSo maybe we will filter out these features in our feature selection part.","metadata":{}},{"cell_type":"code","source":"train['id_11'].value_counts(dropna=False, normalize=True).head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:21.186656Z","iopub.execute_input":"2021-11-13T11:45:21.186928Z","iopub.status.idle":"2021-11-13T11:45:21.205361Z","shell.execute_reply.started":"2021-11-13T11:45:21.18688Z","shell.execute_reply":"2021-11-13T11:45:21.204517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"22% of values in `id_11` are equal to 100 and 76% are missing. Quite strange.","metadata":{}},{"cell_type":"code","source":"plt.hist(train['id_07']);\nplt.title('Distribution of id_07 variable');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:21.206926Z","iopub.execute_input":"2021-11-13T11:45:21.207371Z","iopub.status.idle":"2021-11-13T11:45:21.495482Z","shell.execute_reply.started":"2021-11-13T11:45:21.207184Z","shell.execute_reply":"2021-11-13T11:45:21.493998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train['id_02']);\nplt.title('Distribution of id_02 variable');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:21.500671Z","iopub.execute_input":"2021-11-13T11:45:21.501149Z","iopub.status.idle":"2021-11-13T11:45:21.798155Z","shell.execute_reply.started":"2021-11-13T11:45:21.500959Z","shell.execute_reply":"2021-11-13T11:45:21.797264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train['id_08']);\nplt.title('Distribution of id_08 variable');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:21.799723Z","iopub.execute_input":"2021-11-13T11:45:21.800236Z","iopub.status.idle":"2021-11-13T11:45:22.118665Z","shell.execute_reply.started":"2021-11-13T11:45:21.800184Z","shell.execute_reply":"2021-11-13T11:45:22.117802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of features seem to be normalized, and some are not. So if someone wants to normalize all variables, it would be necessary to separate such variables which seem to be already normalized.","metadata":{}},{"cell_type":"markdown","source":"### 4.1.2 EDA on Categorical Features (id_12 - id_38; DeviceType; DeviceInfo) - Identity","metadata":{}},{"cell_type":"code","source":"train[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']].describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:22.120152Z","iopub.execute_input":"2021-11-13T11:45:22.120667Z","iopub.status.idle":"2021-11-13T11:45:25.127276Z","shell.execute_reply.started":"2021-11-13T11:45:22.120617Z","shell.execute_reply":"2021-11-13T11:45:25.126532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"charts = {}\nfor i in ['id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['id_12'] | charts['id_15'] | charts['id_16']) & (charts['id_28'] | charts['id_29'] | charts['id_32']) & (charts['id_34'] | charts['id_35'] | charts['id_36']) & (charts['id_37'] | charts['id_38']))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T11:45:25.128639Z","iopub.execute_input":"2021-11-13T11:45:25.129623Z","iopub.status.idle":"2021-11-13T11:45:26.071517Z","shell.execute_reply.started":"2021-11-13T11:45:25.128877Z","shell.execute_reply":"2021-11-13T11:45:26.070702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have several features showing some kind of \"found\" status and several binary columns.","metadata":{}},{"cell_type":"code","source":"charts = {}\nfor i in ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n    \nrender(charts['id_30'] & charts['id_31'] & charts['id_33'] & charts['DeviceType'] & charts['DeviceInfo'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T11:45:26.072978Z","iopub.execute_input":"2021-11-13T11:45:26.073512Z","iopub.status.idle":"2021-11-13T11:45:26.437432Z","shell.execute_reply.started":"2021-11-13T11:45:26.073457Z","shell.execute_reply":"2021-11-13T11:45:26.436644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see some information about client's device. It is important to be careful here - some of info could be for old devices and may be absent from test data.","metadata":{}},{"cell_type":"markdown","source":"## 4.2 EDA on Transaction Data\nNow let's have a look at transaction data.\n\n*Previously*:\n- **Numerical Features - Transaction**\n    - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n    - TransactionAmt: transaction payment amount in USD\n    - dist: distance\n    - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n    - D1-D15: timedelta, such as days between previous transaction, etc.\n    - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n​\n​\n- **Categorical Features - Transaction**\n    - ProductCD: product code, the product for each transaction\n    - card1 - card6: : payment card information, such as card type, card category, issue bank, country, etc.\n    - addr1, addr2: address\n    - P_emaildomain: purchaser email domain\n    - R_emaildomain: recipient email domain\n    - M1 - M9: match, such as names on card and address, etc.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1 EDA on Numerical Features (TransactionDT; TransactionAmt; dist; C1-C14; D1-D15; Vxxx) - Transaction","metadata":{}},{"cell_type":"code","source":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:26.438844Z","iopub.execute_input":"2021-11-13T11:45:26.439338Z","iopub.status.idle":"2021-11-13T11:45:26.850006Z","shell.execute_reply.started":"2021-11-13T11:45:26.439106Z","shell.execute_reply":"2021-11-13T11:45:26.848936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A very important idea: it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation.\nThis was already noted in abother kernel: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda","metadata":{}},{"cell_type":"markdown","source":"#### Normalize D Columns\nThe D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past. This will stop the D columns from increasing with time. The formula is D15n = Transaction_Day - D15 and Transaction_Day = TransactionDT/(24*60*60). Afterward we multiple this number by negative one.","metadata":{}},{"cell_type":"code","source":"# PLOT ORIGINAL D\nplt.figure(figsize=(15,5))\nplt.scatter(train.TransactionDT,train.D15)\nplt.title('Original D15')\nplt.xlabel('Time')\nplt.ylabel('D15')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:26.85524Z","iopub.execute_input":"2021-11-13T11:45:26.855769Z","iopub.status.idle":"2021-11-13T11:45:32.723093Z","shell.execute_reply.started":"2021-11-13T11:45:26.855541Z","shell.execute_reply":"2021-11-13T11:45:32.722317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    train['D'+str(i)] =  train['D'+str(i)] - train.TransactionDT/np.float32(24*60*60)\n    test['D'+str(i)] = test['D'+str(i)] - test.TransactionDT/np.float32(24*60*60) ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:32.724544Z","iopub.execute_input":"2021-11-13T11:45:32.725065Z","iopub.status.idle":"2021-11-13T11:45:33.617209Z","shell.execute_reply.started":"2021-11-13T11:45:32.725013Z","shell.execute_reply":"2021-11-13T11:45:33.616482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT TRANSFORMED D\nplt.figure(figsize=(15,5))\nplt.scatter(train.TransactionDT,train.D15)\nplt.title('Transformed D15')\nplt.xlabel('Time')\nplt.ylabel('D15n')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:33.618573Z","iopub.execute_input":"2021-11-13T11:45:33.618853Z","iopub.status.idle":"2021-11-13T11:45:39.633529Z","shell.execute_reply.started":"2021-11-13T11:45:33.618806Z","shell.execute_reply":"2021-11-13T11:45:39.632733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train['TransactionAmt'], label='train');\nplt.hist(test['TransactionAmt'], label='test');\nplt.legend();\nplt.title('Distribution of transaction amount');","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:39.635141Z","iopub.execute_input":"2021-11-13T11:45:39.635698Z","iopub.status.idle":"2021-11-13T11:45:40.114915Z","shell.execute_reply.started":"2021-11-13T11:45:39.635645Z","shell.execute_reply":"2021-11-13T11:45:40.114018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### EDA for columns Vxxx\nPlease refer to https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id.\nAnd 219 V columns were determined redundant by correlation analysis and removed at the beginning of this kernel.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.2 EDA on Categorical Features (ProductCD; card1-card6; addr1, addr2; P_emaildomain; R_emaildomain; M1-M9) - Transaction","metadata":{}},{"cell_type":"code","source":"charts = {}\nfor i in ['ProductCD', 'card4', 'card6', 'M4', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n                x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=400)\n    charts[i] = chart                         \n    \nrender((charts['ProductCD'] | charts['card4']) & (charts['card6'] | charts['M4']) & (charts['card6'] | charts['M4']) & (charts['M1'] | charts['M2']) & (charts['M3'] | charts['M5']) & (charts['M6'] | charts['M7']) & (charts['M8'] | charts['M9']))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T11:45:40.116454Z","iopub.execute_input":"2021-11-13T11:45:40.11697Z","iopub.status.idle":"2021-11-13T11:45:41.412297Z","shell.execute_reply.started":"2021-11-13T11:45:40.11692Z","shell.execute_reply":"2021-11-13T11:45:41.411568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So `card6` is type of card, `card4` is credit card company","metadata":{}},{"cell_type":"code","source":"charts = {}\nfor i in ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2']:\n    feature_count = train[i].value_counts(dropna=False).reset_index()[:40].rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=600)\n    charts[i] = chart\n    \nrender((charts['P_emaildomain'] | charts['R_emaildomain']) & (charts['card1'] | charts['card2']) & (charts['card3'] | charts['card5']) & (charts['addr1'] | charts['addr2']))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T11:45:41.413698Z","iopub.execute_input":"2021-11-13T11:45:41.414204Z","iopub.status.idle":"2021-11-13T11:45:41.763691Z","shell.execute_reply.started":"2021-11-13T11:45:41.414154Z","shell.execute_reply":"2021-11-13T11:45:41.762748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"## Mapping Email","metadata":{}},{"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:41.765291Z","iopub.execute_input":"2021-11-13T11:45:41.765723Z","iopub.status.idle":"2021-11-13T11:45:44.771585Z","shell.execute_reply.started":"2021-11-13T11:45:41.76554Z","shell.execute_reply":"2021-11-13T11:45:44.770854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Functions\nBelow are 5 encoding functions. (1) encode_FE does frequency encoding where it combines train and test first and then encodes. (2) encode_LE is a label encoded for categorical features (3) encode_AG makes aggregated features such as aggregated mean and std (4) encode_CB combines two columns (5) encode_AG2 makes aggregated features where it counts how many unique values of one feature is within a group.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600/notebook#Load-Data\n# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=train,test=test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=train,df2=test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=train, test_df=test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:44.77316Z","iopub.execute_input":"2021-11-13T11:45:44.773439Z","iopub.status.idle":"2021-11-13T11:45:44.812909Z","shell.execute_reply.started":"2021-11-13T11:45:44.773394Z","shell.execute_reply":"2021-11-13T11:45:44.81162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Feature engineering on Numerial Variables\nLet's do some aggregation on top features found in EDA.","metadata":{}},{"cell_type":"code","source":"train['Trans_min_mean'] = train['TransactionAmt'] - train['TransactionAmt'].mean()\ntrain['Trans_min_std'] = train['Trans_min_mean'] / train['TransactionAmt'].std()\ntest['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\ntest['Trans_min_std'] = test['Trans_min_mean'] / test['TransactionAmt'].std()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:44.815126Z","iopub.execute_input":"2021-11-13T11:45:44.815701Z","iopub.status.idle":"2021-11-13T11:45:45.055356Z","shell.execute_reply.started":"2021-11-13T11:45:44.815646Z","shell.execute_reply":"2021-11-13T11:45:45.054421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TransactionAmt_log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_log'] = np.log(test['TransactionAmt'])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:45.056905Z","iopub.execute_input":"2021-11-13T11:45:45.057511Z","iopub.status.idle":"2021-11-13T11:45:45.105835Z","shell.execute_reply.started":"2021-11-13T11:45:45.057452Z","shell.execute_reply":"2021-11-13T11:45:45.105045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\n# train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n# train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n# train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n# train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\n# test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n# test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n# test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n# test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:45.115792Z","iopub.execute_input":"2021-11-13T11:45:45.118603Z","iopub.status.idle":"2021-11-13T11:45:47.773998Z","shell.execute_reply.started":"2021-11-13T11:45:45.116043Z","shell.execute_reply":"2021-11-13T11:45:47.773143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:47.778871Z","iopub.execute_input":"2021-11-13T11:45:47.781226Z","iopub.status.idle":"2021-11-13T11:45:57.439687Z","shell.execute_reply.started":"2021-11-13T11:45:47.781171Z","shell.execute_reply":"2021-11-13T11:45:57.43875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the following features where chosen because each increase local validation. The procedure for engineering features is as follows. First you think of an idea and create a new feature. Then you add it to your model and evaluate whether local validation AUC increases or decreases. If AUC increases keep the feature, otherwise discard the feature.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600/notebook#Load-Data\n\n%time\n# TRANSACTION AMT CENTS\ntrain['cents'] = (train['TransactionAmt'] - np.floor(train['TransactionAmt'])).astype('float32')\ntest['cents'] = (test['TransactionAmt'] - np.floor(test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:45:57.441783Z","iopub.execute_input":"2021-11-13T11:45:57.442276Z","iopub.status.idle":"2021-11-13T11:46:13.004682Z","shell.execute_reply.started":"2021-11-13T11:45:57.442193Z","shell.execute_reply":"2021-11-13T11:46:13.003027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Feature engineering on Categorical Variables Based on Cardinality\nLet's encode the categorical variables based on cardinality!\n- Low Cardinality features: Label Encoding\n- High Cardinality features: Target Encoding / Drop\n","metadata":{}},{"cell_type":"markdown","source":"### Prepare the Data for futher Feature Engineering\n","metadata":{}},{"cell_type":"code","source":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:13.006405Z","iopub.execute_input":"2021-11-13T11:46:13.00691Z","iopub.status.idle":"2021-11-13T11:46:16.837703Z","shell.execute_reply.started":"2021-11-13T11:46:13.00672Z","shell.execute_reply":"2021-11-13T11:46:16.836842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:16.839244Z","iopub.execute_input":"2021-11-13T11:46:16.839736Z","iopub.status.idle":"2021-11-13T11:46:28.722352Z","shell.execute_reply.started":"2021-11-13T11:46:16.839636Z","shell.execute_reply":"2021-11-13T11:46:28.72135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\ncols_to_drop.remove('isFraud')\nlen(cols_to_drop)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:28.723711Z","iopub.execute_input":"2021-11-13T11:46:28.724009Z","iopub.status.idle":"2021-11-13T11:46:28.730178Z","shell.execute_reply.started":"2021-11-13T11:46:28.723962Z","shell.execute_reply":"2021-11-13T11:46:28.729078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:28.731488Z","iopub.execute_input":"2021-11-13T11:46:28.732011Z","iopub.status.idle":"2021-11-13T11:46:29.609307Z","shell.execute_reply.started":"2021-11-13T11:46:28.731957Z","shell.execute_reply":"2021-11-13T11:46:29.608625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# by https://www.kaggle.com/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\ntrain = clean_inf_nan(train)\ntest = clean_inf_nan(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:29.611471Z","iopub.execute_input":"2021-11-13T11:46:29.61202Z","iopub.status.idle":"2021-11-13T11:46:40.82631Z","shell.execute_reply.started":"2021-11-13T11:46:29.611969Z","shell.execute_reply":"2021-11-13T11:46:40.825629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split into Categorical and Numerical dataset","metadata":{}},{"cell_type":"code","source":"# All Categorial Columns\nidentity_cat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n                     'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n                     'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n                     'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\ntransaction_cat_cols = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2',\n                        'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3',\n                        'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3','P_emaildomain', 'R_emaildomain',\n                        'P_emaildomain_bin', 'R_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_suffix',\n                        'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n\ncat_cols = identity_cat_cols + transaction_cat_cols\ncat_cols = list(set(cat_cols) & set(train.columns)) # because maybe we have dropped some categorical columns\nencoded_cols = ['addr1','card1','card2','card3']\ncat_cols = list(set(cat_cols) - set(encoded_cols))\n\ncommon_cat_cols = [col for col in test.columns if col in cat_cols]\n\ncat_cols == common_cat_cols","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:40.827565Z","iopub.execute_input":"2021-11-13T11:46:40.827835Z","iopub.status.idle":"2021-11-13T11:46:40.842175Z","shell.execute_reply.started":"2021-11-13T11:46:40.827789Z","shell.execute_reply":"2021-11-13T11:46:40.840968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:40.843717Z","iopub.execute_input":"2021-11-13T11:46:40.84423Z","iopub.status.idle":"2021-11-13T11:46:40.851201Z","shell.execute_reply.started":"2021-11-13T11:46:40.84396Z","shell.execute_reply":"2021-11-13T11:46:40.850133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(common_cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:40.852755Z","iopub.execute_input":"2021-11-13T11:46:40.853168Z","iopub.status.idle":"2021-11-13T11:46:40.859463Z","shell.execute_reply.started":"2021-11-13T11:46:40.852998Z","shell.execute_reply":"2021-11-13T11:46:40.858386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop the categorial features that do not exist in the test dataset.","metadata":{}},{"cell_type":"code","source":"cat_train = train[common_cat_cols]\nnum_train = train.drop(cat_cols, axis=1)\n\ncat_test = test[common_cat_cols]\nnum_test = test.drop(common_cat_cols, axis=1)\n\ncat_train.columns == cat_test.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:40.860898Z","iopub.execute_input":"2021-11-13T11:46:40.861391Z","iopub.status.idle":"2021-11-13T11:46:41.51371Z","shell.execute_reply.started":"2021-11-13T11:46:40.861144Z","shell.execute_reply":"2021-11-13T11:46:41.512963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigating Cardinality","metadata":{}},{"cell_type":"code","source":"# Get the number of unique entries in each column with categorical data\ncat_nunique = list(map(lambda col: cat_train[col].nunique(), common_cat_cols))\nd = dict(zip(common_cat_cols, cat_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x:x[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:41.515038Z","iopub.execute_input":"2021-11-13T11:46:41.515326Z","iopub.status.idle":"2021-11-13T11:46:43.604737Z","shell.execute_reply.started":"2021-11-13T11:46:41.515254Z","shell.execute_reply":"2021-11-13T11:46:43.603825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the categorical columns' types to Category\ncat_train = cat_train.astype('category')\ncat_test = cat_test.astype('category')\n\nprint(cat_train.dtypes)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:43.606413Z","iopub.execute_input":"2021-11-13T11:46:43.606838Z","iopub.status.idle":"2021-11-13T11:46:47.604041Z","shell.execute_reply.started":"2021-11-13T11:46:43.606662Z","shell.execute_reply":"2021-11-13T11:46:47.603224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns that will be label-encoded\nlow_cardinality_cols = [col for col in common_cat_cols if cat_train[col].nunique() < 10] # 10 is a number chosen by discretion\n\n# Columns that will be target encoded (or dropped eventally)\nhigh_cardinality_cols = list(set(common_cat_cols) - set(low_cardinality_cols))\n\nprint('Categorical columns that will be label encoded: ', low_cardinality_cols)\nprint('\\nCategorical columns that will be target encoded (or dropped eventally): ', high_cardinality_cols)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:47.605461Z","iopub.execute_input":"2021-11-13T11:46:47.605992Z","iopub.status.idle":"2021-11-13T11:46:47.773508Z","shell.execute_reply.started":"2021-11-13T11:46:47.605939Z","shell.execute_reply":"2021-11-13T11:46:47.772709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LE_train = cat_train[low_cardinality_cols]\nLE_test = cat_test[low_cardinality_cols]\n\nprint(LE_train.describe())\nprint(\"---------------\")\nprint(LE_test.describe())","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:47.77475Z","iopub.execute_input":"2021-11-13T11:46:47.775202Z","iopub.status.idle":"2021-11-13T11:46:48.207874Z","shell.execute_reply.started":"2021-11-13T11:46:47.77515Z","shell.execute_reply":"2021-11-13T11:46:48.207044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encode the data\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in low_cardinality_cols:\n    LEncoder = LabelEncoder()\n    LEncoder.fit(list(LE_train[col].astype(str).values) + list(LE_test[col].astype(str).values))\n    LE_train[col] = LEncoder.transform(list(LE_train[col].astype(str).values))\n    LE_test[col] = LEncoder.transform(list(LE_test[col].astype(str).values))\n    \ntrain = num_train.merge(LE_train, how='left', left_index=True, right_index=True)\ntest = num_test.merge(LE_test, how='left', left_index=True, right_index=True)\n\nprint(train[low_cardinality_cols].describe())\nprint(\"---------------\")\nprint(test[low_cardinality_cols].describe())","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:46:48.209245Z","iopub.execute_input":"2021-11-13T11:46:48.209798Z","iopub.status.idle":"2021-11-13T11:47:54.687871Z","shell.execute_reply.started":"2021-11-13T11:46:48.209744Z","shell.execute_reply":"2021-11-13T11:47:54.687094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I actually tried target encoding the high cardinality columns, but the local validation turned out to be better off when we just drop them instead. So the following code blocks for target encoding are commented out.**","metadata":{}},{"cell_type":"code","source":"# high_cardinality_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_train[high_cardinality_cols].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataset has enough rows to do the following encoding split and label encoding!","metadata":{}},{"cell_type":"code","source":"# print(cat_train[high_cardinality_cols].describe())\n# print(\"---------------\")\n# print(cat_test[high_cardinality_cols].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # target encode the data\n# from category_encoders import MEstimateEncoder\n# TE_train = cat_train[high_cardinality_cols] \n# TE_train = TE_train.merge(train['isFraud'], how='left', left_index=True, right_index=True)\n# TE_test = cat_test[high_cardinality_cols]\n\n# # encoding split\n# X_encode = TE_train.sample(frac=0.3, random_state=0)\n# y_encode = X_encode.pop(\"isFraud\")\n\n# # training split\n# X_pretrain = TE_train.drop(X_encode.index)\n# y_train = X_pretrain.pop(\"isFraud\")\n\n# #apply M-Estimate encoding (the choice of m is based on our previous cardinality investigation)\n# for col in high_cardinality_cols:\n#     if TE_train[col].nunique() < 100:\n#         m = 0.5\n#     elif TE_train[col].nunique() < 550:\n#         m = 2.5\n#     else:\n#         m = 5\n#     encoder = MEstimateEncoder(cols=[col], m=m)\n#     encoder.fit(X_encode, y_encode)\n#     X_pretrain = encoder.transform(X_pretrain)\n#     TE_test = encoder.transform(TE_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(X_pretrain[high_cardinality_cols].describe())\n# print(\"---------------\")\n# print(TE_test[high_cardinality_cols].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_pretrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = X_pretrain.merge(train, how='left', left_index=True, right_index=True)\n# test = TE_test.merge(test, how='left', left_index=True, right_index=True)\n\n# print(train[common_cat_cols].describe())\n# print(\"---------------\")\n# print(test[common_cat_cols].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del num_train, num_test, LE_train, LE_test#, TE_train, TE_test","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.689232Z","iopub.execute_input":"2021-11-13T11:47:54.689778Z","iopub.status.idle":"2021-11-13T11:47:54.699321Z","shell.execute_reply.started":"2021-11-13T11:47:54.689726Z","shell.execute_reply":"2021-11-13T11:47:54.698693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Feature Selection - Time Consistency\nAfter the above feature engineering, we've come a long way!\nNow, we have 205 features in the train data. We will now check each of our  for \"time consistency\". \n\nThanks to https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600/notebook, respective mdoels have been built. Each model is trained on the first month of the training data and only use one feature. The model then predicts the last month of the training data. We want both training AUC and validation AUC to be above AUC = 0.5. It turns out that 19 features fail this test so we will remove them. Additionally we will remove 7 D columns that are mostly NAN. More techniques for feature selection are listed at https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308.","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.700999Z","iopub.execute_input":"2021-11-13T11:47:54.701405Z","iopub.status.idle":"2021-11-13T11:47:54.713934Z","shell.execute_reply.started":"2021-11-13T11:47:54.701233Z","shell.execute_reply":"2021-11-13T11:47:54.713207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.715633Z","iopub.execute_input":"2021-11-13T11:47:54.716152Z","iopub.status.idle":"2021-11-13T11:47:54.723041Z","shell.execute_reply.started":"2021-11-13T11:47:54.716102Z","shell.execute_reply":"2021-11-13T11:47:54.722217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(train.columns)\n\n# Remove 7 D columns that are mostly NAN\nfor c in list(set(['D6','D7','D8','D9','D12','D13','D14']) & set(cols)):\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in list(set(['C3','M5','id_08','id_33']) & set(cols)):\n    cols.remove(c)\nfor c in list(set(['card4','id_07','id_14','id_21','id_30','id_32','id_34']) & set(cols)):\n    cols.remove(c)\nfor c in list(set(['id_'+str(x) for x in range(22,28)]) & set(cols)):\n    cols.remove(c)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.724342Z","iopub.execute_input":"2021-11-13T11:47:54.72495Z","iopub.status.idle":"2021-11-13T11:47:54.734393Z","shell.execute_reply.started":"2021-11-13T11:47:54.724898Z","shell.execute_reply":"2021-11-13T11:47:54.733817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.736392Z","iopub.execute_input":"2021-11-13T11:47:54.736887Z","iopub.status.idle":"2021-11-13T11:47:54.748432Z","shell.execute_reply.started":"2021-11-13T11:47:54.736837Z","shell.execute_reply":"2021-11-13T11:47:54.747579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train = train[cols].copy()\nfinal_test = test[list(set(cols) - set(['isFraud']))].copy()\n\nprint(f'Our Final Train dataset has {final_train.shape[0]} rows and {final_train.shape[1]} columns.')\nprint(f'Our Final Test dataset has {final_test.shape[0]} rows and {final_test.shape[1]} columns.')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:54.7497Z","iopub.execute_input":"2021-11-13T11:47:54.750251Z","iopub.status.idle":"2021-11-13T11:47:55.41759Z","shell.execute_reply.started":"2021-11-13T11:47:54.750204Z","shell.execute_reply":"2021-11-13T11:47:55.416765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train, test","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:55.418813Z","iopub.execute_input":"2021-11-13T11:47:55.419253Z","iopub.status.idle":"2021-11-13T11:47:55.450946Z","shell.execute_reply.started":"2021-11-13T11:47:55.419201Z","shell.execute_reply":"2021-11-13T11:47:55.450132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. XGBoost Modelling\nHooray! We have 195 final features!\n\nLet's continue with XGBoost Modelling!","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Local Validation\nFor this competition, we will use local validation. I evaluated features by training on the first 75% of the train data and predicting the last 25% of the train data. ","metadata":{}},{"cell_type":"code","source":"X_train = final_train.copy()\ny_train = X_train.pop('isFraud')\nX_test = final_test.copy()\nX_train,X_test = X_train.align(other=X_test,join='left', axis=1)\n\nprint(X_train.describe())\nprint(X_test.describe())","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:47:55.453607Z","iopub.execute_input":"2021-11-13T11:47:55.454052Z","iopub.status.idle":"2021-11-13T11:48:23.447003Z","shell.execute_reply.started":"2021-11-13T11:47:55.454001Z","shell.execute_reply":"2021-11-13T11:48:23.446017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idxT = X_train.index[:3*len(X_train)//4]\nidxV = X_train.index[3*len(X_train)//4:]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:48:23.44848Z","iopub.execute_input":"2021-11-13T11:48:23.449032Z","iopub.status.idle":"2021-11-13T11:48:23.455393Z","shell.execute_reply.started":"2021-11-13T11:48:23.44897Z","shell.execute_reply":"2021-11-13T11:48:23.454322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()/y_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:48:23.457102Z","iopub.execute_input":"2021-11-13T11:48:23.457626Z","iopub.status.idle":"2021-11-13T11:48:23.480255Z","shell.execute_reply.started":"2021-11-13T11:48:23.457393Z","shell.execute_reply":"2021-11-13T11:48:23.479478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We face the imbalance data problem, but we will solve it by subsampling!","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nclf = xgb.XGBClassifier( \n    n_estimators=2000,\n    max_depth=12, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    #missing=-1, \n    eval_metric='auc',\n    # USE CPU\n    #nthread=4,\n    #tree_method='hist' \n    # USE GPU\n    tree_method='gpu_hist' \n)\n\nh = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n            early_stopping_rounds=100,\n            eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n            verbose=50)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:48:23.481501Z","iopub.execute_input":"2021-11-13T11:48:23.481828Z","iopub.status.idle":"2021-11-13T11:49:26.734527Z","shell.execute_reply.started":"2021-11-13T11:48:23.481777Z","shell.execute_reply":"2021-11-13T11:49:26.73383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB Most Important Features')\nplt.tight_layout()\nplt.show()\ndel clf, h; x=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:49:26.735987Z","iopub.execute_input":"2021-11-13T11:49:26.736253Z","iopub.status.idle":"2021-11-13T11:49:30.355396Z","shell.execute_reply.started":"2021-11-13T11:49:26.736207Z","shell.execute_reply":"2021-11-13T11:49:30.354528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 XGB Hyperopt\nReference: https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt/notebook","metadata":{}},{"cell_type":"markdown","source":"### Defining the HyperOpt function with parameters space and model","metadata":{}},{"cell_type":"code","source":"X_train = final_train.copy()\ny_train = X_train.pop('isFraud')\nX_test = final_test.copy()\nX_train,X_test = X_train.align(other=X_test,join='left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:43:14.829604Z","iopub.execute_input":"2021-11-13T01:43:14.829896Z","iopub.status.idle":"2021-11-13T01:43:16.034391Z","shell.execute_reply.started":"2021-11-13T01:43:14.829846Z","shell.execute_reply":"2021-11-13T01:43:16.033625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sub.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=2000, random_state=4, verbose=True, \n            tree_method='gpu_hist', early_stopping_rounds=100,\n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-13T06:33:58.153373Z","iopub.execute_input":"2021-11-13T06:33:58.153867Z","iopub.status.idle":"2021-11-13T06:33:58.178865Z","shell.execute_reply.started":"2021-11-13T06:33:58.153669Z","shell.execute_reply":"2021-11-13T06:33:58.178087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set algoritm parameters\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=25)\n\n# Print best parameters\nbest_params = space_eval(space, best)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T06:33:58.180349Z","iopub.execute_input":"2021-11-13T06:33:58.180876Z","iopub.status.idle":"2021-11-13T11:19:25.154338Z","shell.execute_reply.started":"2021-11-13T06:33:58.180707Z","shell.execute_reply":"2021-11-13T11:19:25.152811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3 Trainning and Predicting with best Parameters\n","metadata":{}},{"cell_type":"markdown","source":"### Predicting X test","metadata":{}},{"cell_type":"code","source":"best_params['max_depth'] = int(best_params['max_depth'])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:19:25.156651Z","iopub.execute_input":"2021-11-13T11:19:25.157122Z","iopub.status.idle":"2021-11-13T11:19:25.161718Z","shell.execute_reply.started":"2021-11-13T11:19:25.157068Z","shell.execute_reply":"2021-11-13T11:19:25.160813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:19:25.163139Z","iopub.execute_input":"2021-11-13T11:19:25.163672Z","iopub.status.idle":"2021-11-13T11:19:25.177734Z","shell.execute_reply.started":"2021-11-13T11:19:25.163415Z","shell.execute_reply":"2021-11-13T11:19:25.177085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Following is our best parameters derived from the XGB HyperOpt.\n\n#### Just to make our kernel run faster(the HyperOpt process takes 4:45:26), the previous XGB HyperOpt have been commented out.","metadata":{}},{"cell_type":"code","source":"best_params = {'bagging_fraction': 0.5818772519688797,\n 'colsample_bytree': 0.3035307099891744,\n 'feature_fraction': 0.795967379488282,\n 'gamma': 0.6896677451866189,\n 'learning_rate': 0.011336192527320772,\n 'max_depth': 20,\n 'min_child_samples': 140,\n 'num_leaves': 230,\n 'reg_alpha': 0.06035695642758,\n 'reg_lambda': 0.012734543098346575,\n 'subsample': 0.7}","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:49:30.357031Z","iopub.execute_input":"2021-11-13T11:49:30.357538Z","iopub.status.idle":"2021-11-13T11:49:30.365176Z","shell.execute_reply.started":"2021-11-13T11:49:30.357304Z","shell.execute_reply":"2021-11-13T11:49:30.363242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = final_train.copy()\ny_train = X_train.pop('isFraud')\nX_test = final_test.copy()\nX_train,X_test = X_train.align(other=X_test,join='left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:49:30.366725Z","iopub.execute_input":"2021-11-13T11:49:30.367244Z","iopub.status.idle":"2021-11-13T11:49:31.46533Z","shell.execute_reply.started":"2021-11-13T11:49:30.367014Z","shell.execute_reply":"2021-11-13T11:49:31.464641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"XGBoost version:\", xgb.__version__)\n\nclf = xgb.XGBClassifier(\n    n_estimators=2000,\n    **best_params,\n    tree_method='gpu_hist',\n    verbose=50,\n    early_stopping_rounds=100\n)\n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:49:31.466627Z","iopub.execute_input":"2021-11-13T11:49:31.466901Z","iopub.status.idle":"2021-11-13T11:56:08.83892Z","shell.execute_reply.started":"2021-11-13T11:49:31.466855Z","shell.execute_reply":"2021-11-13T11:56:08.838007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGB Most Important Features')\nplt.tight_layout()\nplt.show()\ndel clf; x=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:56:08.840233Z","iopub.execute_input":"2021-11-13T11:56:08.840508Z","iopub.status.idle":"2021-11-13T11:56:30.489896Z","shell.execute_reply.started":"2021-11-13T11:56:08.840463Z","shell.execute_reply":"2021-11-13T11:56:30.488998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seting y_pred to csv","metadata":{}},{"cell_type":"code","source":"sub['isFraud'] = y_preds\nsub.to_csv('XGB_hypopt_model.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:56:30.491346Z","iopub.execute_input":"2021-11-13T11:56:30.491653Z","iopub.status.idle":"2021-11-13T11:56:32.893288Z","shell.execute_reply.started":"2021-11-13T11:56:30.491604Z","shell.execute_reply":"2021-11-13T11:56:32.892579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(sub.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGB Submission')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T11:56:32.896745Z","iopub.execute_input":"2021-11-13T11:56:32.896986Z","iopub.status.idle":"2021-11-13T11:56:33.317428Z","shell.execute_reply.started":"2021-11-13T11:56:32.89694Z","shell.execute_reply":"2021-11-13T11:56:33.31636Z"},"trusted":true},"execution_count":null,"outputs":[]}]}